HISTORY OF SEARCH ENGINES

For the longest time, before the digital revolution, humans sought information through other human beings. Be it books in the library, recommendation for a good restaurant or reliability of a product, human beings turned to other human beings for information. With the advent of technology, the amount of information grew and so did our ways to store and access it.
The story of search engines started at the Cornell University before the Internet was created.  In the 1960s, Gerard Salton (now called the ‘father of modern search technology’) and his colleagues at Cornell developed the SMART Information Retrieval System.  This early information retrieval system proved to be the basis of many future search engines with important concepts like term weighting, relevance feedback, term dependency, vector space model, Inverse Document Frequency (IDF), Term Frequency (TF), and a lot more.
In 1990, the internet was created. Around the same time, came the first search engine: Archie. Archie created a searchable database of filenames. It indexed the contents of public FTP servers and the user could query the name of the files on its server. 
The first few hundred webpages didn’t exist till 1993. In 1994, Berners-Lee founded the World Wide Web Consortium (W3C) at the Massachusetts Institute of Technology. Then came the first web’s robot: World Wide Web Wanderer. Soon, bot fed search engines had surfaced on the web. JumpStation, the World Wide Web Worm, the Repository-Based Software Engineering (RBSE) spider and more, used bots to crawl web pages on the internet, attempting to rank them. Most of primitive ranking was done using the words on the pages. So, in order to rank a page better for some topic, web pages would have the topic word many many times, on them. This, inevitably, lead to spamming. 
In the next series of search engines, off-the-page factors started being examined for ranking, since all on-page information was in control of the publisher. The idea was to determine the quality of the page based on metrics that would make search results, less spammable. To consider what people other than the publisher had to say, so to speak.
One of the popular search engines Yahoo! was actually a directory. A search engine, essentially, uses automated search algorithms (crawler-based), while directories are manually compiled. Yahoo! provided a human compiled description with each URL, and for the longest time, was a directory.
 Google started using link analysis (an off-page factor) where links were considered as votes. For example, if you find a link of page B on page A, that is considered as an endorsement of page B by page A. It used link analysis to rank the pages, considering more votes meant better quality, better ranking. One of the other off-page factor, called click through (counting the number of time, a certain link was clicked) was used by a search engine DirectHit. Their idea was more clicks, better ranking. 
To this day, we use link analysis and off-page factors for ranking web pages. But as access to internet increased, so did the information. To improve the search results, the idea of ‘vertical search’ was introduced. In vertical search, the results are focused. For example, in horizontal search, the search query would return web pages with news, entertainment, sports, shopping, and so on. In vertical search, the domain of the query is concise, which filters out irrelevant results. Google calls it ‘universal search’. So now, if you search ‘pictures of cat’, it shows pictures of cats, as opposed to links containing them. 
Microsoft launched Bing in 2009 and changed the search landscape by placing inline search suggestions for related searches directly in the result set. For instance, when you search for ‘credit cards’ they will suggest related phrases like ‘credit card debt’, etc.
To further improve the relevance of results, search engines have now started personalizing them. With access to search history, search engines relate various searches over time. So if you’re searching about Bangalore, and then for cheap flights, chances are that the top results would show cheap flights for Bangalore. Another factor being considered for searching is location. The results shown are relevant to the person’s current location. Searching for restaurants would lead results for restaurants near you. 
The future of search engines may revolve around using information from social networks. Analysing online reviews on social platforms like Twitter, Facebook, Google+, etc. could lead to better quality and more relevant results. Also, search engines are now experimenting with voice-based technology. 
Search engines have become an integral part of our lives, with vast information, just clicks away. They have come a long way in the past few decades but they still have a long way to go.
